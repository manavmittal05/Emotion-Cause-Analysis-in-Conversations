{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val_new.json'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "def transform_conversations(data):\n",
    "    transformed_data = []\n",
    "    for conversation in data:\n",
    "        transformed_conversation = {\n",
    "            \"conversation_ID\": conversation[\"conversation_ID\"],\n",
    "            \"conversation\": [],\n",
    "            \"emotion-cause_pairs\": conversation[\"emotion-cause_pairs\"]\n",
    "        }\n",
    "        for utterance in conversation[\"conversation\"]:\n",
    "            transformed_text = f\"{utterance['speaker']} {utterance['emotion']} said: {utterance['text']}\"\n",
    "            transformed_conversation[\"conversation\"].append({\n",
    "                \"utterance_ID\": utterance[\"utterance_ID\"],\n",
    "                \"text\": transformed_text,\n",
    "                \"speaker\": utterance[\"speaker\"],\n",
    "                \"emotion\": utterance[\"emotion\"]\n",
    "            })\n",
    "        transformed_data.append(transformed_conversation)\n",
    "    return transformed_data\n",
    "\n",
    "# extract data from train_file.json\n",
    "\n",
    "import os \n",
    "\n",
    "input_file_path =\"val_file.json\"\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(input_file_path, \"r\") as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "\n",
    "processed_data = transform_conversations(data)\n",
    "\n",
    "output_file_path = \"val_new.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(processed_data, outfile)\n",
    "\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sliding_padded_conversation_with_speaker_bio(conversation, num_padding=5):\n",
    "    processed = []\n",
    "    conversation_texts = [utterance[\"text\"] for utterance in conversation[\"conversation\"]]\n",
    "    speakers = [utterance[\"speaker\"] for utterance in conversation[\"conversation\"]]\n",
    "    num_utterances = len(conversation_texts)\n",
    "    padding_token = \"[PAD]\"\n",
    "    emotion_causes = {}\n",
    "    \n",
    "    for pair in conversation[\"emotion-cause_pairs\"]:\n",
    "        target_id, cause_text = pair[0].split(\"_\")[0], pair[1].split(\"_\")[1]\n",
    "        if target_id in emotion_causes:\n",
    "            emotion_causes[target_id].append(cause_text)\n",
    "        else:\n",
    "            emotion_causes[target_id] = [cause_text]\n",
    "    \n",
    "    for i in range(num_utterances):\n",
    "        utterance_id_str = str(conversation[\"conversation\"][i][\"utterance_ID\"])\n",
    "        left_context_text = ([padding_token] * num_padding + conversation_texts[:i+1])[-(num_padding+1):]\n",
    "        left_context_speakers = ([None] * num_padding + speakers[:i+1])[-(num_padding+1):]\n",
    "        right_context_text = (conversation_texts[i+1:] + [padding_token] * num_padding)[:num_padding]\n",
    "        right_context_speakers = (speakers[i+1:] + ['null'] * num_padding)[:num_padding]\n",
    "        \n",
    "        context_parts = left_context_text + right_context_text\n",
    "        context_speakers = left_context_speakers + right_context_speakers\n",
    "        full_text = \" [SEP] \".join(context_parts)\n",
    "        full_speakers = []\n",
    "        \n",
    "        for part, speaker in zip(context_parts, context_speakers):\n",
    "            full_speakers.extend([speaker] * len(part.split()) + ['null'])\n",
    "        full_speakers.pop()\n",
    "        \n",
    "        tokens = full_text.split()\n",
    "        bio_tags = [\"O\"] * len(tokens)\n",
    "        \n",
    "        if utterance_id_str in emotion_causes:\n",
    "            for cause in emotion_causes[utterance_id_str]:\n",
    "                start_pos = full_text.find(cause)\n",
    "                if start_pos != -1:\n",
    "                    cause_tokens = cause.split()\n",
    "                    start_index = len(full_text[:start_pos].split())\n",
    "                    end_index = start_index + len(cause_tokens)\n",
    "                    if start_index < len(bio_tags):\n",
    "                        bio_tags[start_index] = \"B\"\n",
    "                        for j in range(start_index + 1, end_index):\n",
    "                            if j < len(bio_tags):\n",
    "                                bio_tags[j] = \"I\"\n",
    "        \n",
    "        processed.append({\n",
    "            \"conversation_ID\": conversation[\"conversation_ID\"],\n",
    "            \"utterance_ID\": conversation[\"conversation\"][i][\"utterance_ID\"],\n",
    "            \"padded_text\": full_text,\n",
    "            \"bio_tags\": bio_tags, \n",
    "            \"utterance_emotion\": conversation[\"conversation\"][i][\"emotion\"], \n",
    "            \"utterance_speaker\": conversation[\"conversation\"][i][\"speaker\"],\n",
    "            \"speakers_in_context\": full_speakers\n",
    "        })\n",
    "        \n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in val_file_new.json\n"
     ]
    }
   ],
   "source": [
    "# read data from train_file.json \n",
    "\n",
    "import json \n",
    "\n",
    "input_file_path = \"val_new.json\"\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(input_file_path, \"r\") as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "def process_full_dataset(dataset):\n",
    "    processed_dataset = []\n",
    "    for conversation in dataset:\n",
    "        processed_conversation = process_sliding_padded_conversation_with_speaker_bio(conversation)\n",
    "        processed_dataset.extend(processed_conversation)\n",
    "    return processed_dataset\n",
    "\n",
    "full_dataset = data\n",
    "processed_full_dataset = process_full_dataset(full_dataset)\n",
    "\n",
    "output_file_path = \"val_file_new.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "    json.dump(processed_full_dataset, outfile)\n",
    "\n",
    "print(f\"File saved in {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
